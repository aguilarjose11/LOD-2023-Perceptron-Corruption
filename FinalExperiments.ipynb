{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae4ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Perceptron.perceptron as pn\n",
    "from Perceptron.data_gen import Universe, separable_regression, data_distribution\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, zero_one_loss\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser, RawTextHelpFormatter \n",
    "import random\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## Data Corruption Experiment\n",
    "from typing import List, Tuple\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a934ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def pickle_data(\n",
    "    root_dir, \n",
    "    results,\n",
    "    args):\n",
    "    \n",
    "    # Make sure it is a directory!\n",
    "    if root_dir[-1] != '/':\n",
    "        root_dir += '/'\n",
    "    \n",
    "    # Create pickle structure\n",
    "    pkl = {\n",
    "        'results': dict(results),\n",
    "        'args':    args,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Create file name\n",
    "\n",
    "    file_name = f\"{args.label}_test_size_{args.test_size}.pkl\"\n",
    "    \n",
    "    with open(f\"{root_dir}{file_name}\", 'wb') as pkl_file:\n",
    "        pickle.dump(pkl, pkl_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95f72b-1ed7-4378-aff9-9d17f8f5570a",
   "metadata": {},
   "source": [
    "## Theoretical Machine Learning Functions\n",
    "\n",
    "The functions bellow provide wrappers for better interpreting theory from the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ea4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment required functions\n",
    "def sample_data(\n",
    "    lows:      List[float],\n",
    "    highs:     List[float],\n",
    "    n_samples: int,\n",
    "    seed:      int=None\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"Sample uniform distribution bounded by lows and highs\n",
    "    \n",
    "        Using a uniform distribution, perform sampling over the \n",
    "    distribution such that the space the distribution is sampling will \n",
    "    be bounded by the given bounds from the lows and highs. Lows and \n",
    "    highs will be arrays that contain the minimum and maximum values \n",
    "    per dimension on the data to be samples. For example, if we have 4 \n",
    "    values in both lows and highs, then, at the time of sampling n_samples\n",
    "    samples we will have n_samples of 4 attributes each: (n_samples, 4).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(lows) == len(highs), f\"Non-matching lows and highs: {len(lows) != {len(highs)}}\"\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    data_shape = (n_samples, len(lows)) # See assertion #1\n",
    "    data = rng.uniform(lows, highs, data_shape)\n",
    "    return data\n",
    "\n",
    "# splitting the dataset into bins can be done with: np.split(data, n_buckets)\n",
    "# Recommend shuffling beforehand tho.\n",
    "\n",
    "class Concept:\n",
    "    \"\"\"Label given data\n",
    "    Using a model as truth, label given data.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        return self.model.solve(X)\n",
    "    \n",
    "    \n",
    "class NPolynomial:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n:    int, \n",
    "                 low:  float=0, \n",
    "                 high: float=1, \n",
    "                 seed: int=42\n",
    "                ):\n",
    "        self.n     = n\n",
    "        self.seed  = seed\n",
    "        self.low   = low\n",
    "        self.high  = high\n",
    "        rng        = np.random.default_rng(seed)\n",
    "        self.coeff = rng.uniform(low, high, (n, 1))\n",
    "        self.exps  = [exp for exp in range(n)[::-1]]\n",
    "        \n",
    "    def solve(self, vals):\n",
    "        var = np.power(vals, self.exps)\n",
    "        activation = np.sign(var @ self.coeff)                                        \n",
    "        activation[activation == 0] = -1\n",
    "        return activation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d44b8d-2392-40d8-b684-577fb0f40c5f",
   "metadata": {},
   "source": [
    "## Data Corruption Functions and Experiment\n",
    "\n",
    "These functions are used for carrying out data corruption and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0b324-47fb-4e33-a89b-a1423a67ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Corruption Code for Experiments'''\n",
    "\n",
    "def perceptron_data_corruption(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        test_data,\n",
    "        test_labels,\n",
    "        model_params,\n",
    "        verbose,\n",
    "        history,\n",
    "        seed,\n",
    "        ):\n",
    "        '''Corrupt given bucketized data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        train_data\n",
    "            Training data that is already bucketized\n",
    "        train_labels\n",
    "            Training labels that is already bucketized\n",
    "        test_data\n",
    "            Testing dataset\n",
    "        test_labels\n",
    "            Testing dataset labels\n",
    "        model_params\n",
    "            Perceptron model hyperparameters to train on\n",
    "        verbose\n",
    "            specify verbosity of messages\n",
    "        history\n",
    "            Shall be a dictionary that has initialized key-value pairs.\n",
    "            The keys shall contain all of the buckets to be used. The\n",
    "            values shall be lists that may or may not already contain\n",
    "            scores from previous runs.\n",
    "        seed\n",
    "            Random seed used when choosing indices\n",
    "        '''\n",
    "\n",
    "        # Calculate number of buckets user is passing.\n",
    "        n_buckets = len(train_data) # could add min/max params to specify buckets!\n",
    "        \n",
    "        rng = np.random.default_rng(seed)\n",
    "        L_values = []\n",
    "        \n",
    "        # Begin with high corruption and then add more buckets\n",
    "        for buckets in range(1, n_buckets):\n",
    "\n",
    "            if verbose > 1:\n",
    "                print(f\"\\tBuckets used: {buckets}\")\n",
    "            # Choose buckets to be used.\n",
    "            indices = rng.choice(range(1, len(train_data)), size=buckets, replace=False)\n",
    "            X       = np.concatenate(train_data[indices])\n",
    "            Y       = np.concatenate(train_labels[indices])\n",
    "            if verbose > 2:\n",
    "                print(f\"\\tData points used: {len(X)}\")\n",
    "\n",
    "            # Train model\n",
    "            model = pn.PocketPerceptron(**model_params)\n",
    "            model.train(X, Y)\n",
    "            pred = model.solve(test_data)\n",
    "\n",
    "            # Measure zero-one & store\n",
    "            score = accuracy_score(pred, test_labels)\n",
    "            #score_list.append(score)\n",
    "\n",
    "            if verbose > 3:\n",
    "                print(f\"\\t\\tScore: {score}\")\n",
    "            history[buckets].append(score)\n",
    "            # Used by Gallant's learning bound.\n",
    "            L_values.append(np.linalg.norm(model.W))\n",
    "            \n",
    "        history['L'].append(L_values)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "\n",
    "def perceptron_corruption_experiment(\n",
    "    X,\n",
    "    y,\n",
    "    test_size,\n",
    "    n_buckets,\n",
    "    model_params,\n",
    "    n_runs,\n",
    "    seed,\n",
    "    verbose,\n",
    "    ):\n",
    "    '''Conduct corruption experiment and report results\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    X\n",
    "        Dataset to train/test on.\n",
    "    y\n",
    "        Labels of dataset to train/test on.\n",
    "    test_size\n",
    "        Size of testing dataset to be split into. See StratifiedShuffleSplit f-\n",
    "        rom sklearn.\n",
    "    n_buckets\n",
    "        Number of buckets to split training data into.\n",
    "    model_parameters\n",
    "        Dictionary containing Pocket Perceptron algorithm's constructor parame-\n",
    "        ters. See Perceptron.perceptron.PocketPerceptron for list.\n",
    "    n_runs\n",
    "        Number of experiment iterations where during each iteration data is co-\n",
    "        rrupted progressively.\n",
    "    seed\n",
    "        Random seed for generators used in concept and model initialization.\n",
    "    verbose\n",
    "        Specify verbosity of output.\n",
    "    '''\n",
    "    \n",
    "    # Will have n_runs scores per bucket size used for training.\n",
    "    history   = {buckets: [] for buckets in range(1, n_buckets)}\n",
    "    # Magnitude of learned vector. Will contain lists of magnitudes.\n",
    "    history['L'] = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        if verbose > 0:\n",
    "            print(f\"Run #{run}\")\n",
    "            \n",
    "        '''Creation of training/testing datasets (bucketized)'''\n",
    "        sss = StratifiedShuffleSplit(\n",
    "            n_splits=1, \n",
    "            test_size=test_size, \n",
    "            random_state=seed + run # This way data is shuffled differently every run!\n",
    "        )\n",
    "        for train_i, test_i in sss.split(X, y):\n",
    "            train_data, train_labels = X[train_i], y[train_i]\n",
    "            test_data, test_labels = X[test_i], y[test_i]\n",
    "        # We just need to bucketize the training data now (Testing data used as is)\n",
    "        train_data   = np.array_split(train_data, n_buckets) # split rises exception if not even!\n",
    "        train_data   = np.array(train_data) # Helps in keeping bucket structure\n",
    "        train_labels = np.array_split(train_labels, n_buckets)\n",
    "        train_labels = np.array(train_labels)\n",
    "\n",
    "\n",
    "        ''' Conduct corruption and obtain scores '''\n",
    "        history = perceptron_data_corruption(\n",
    "            train_data,\n",
    "            train_labels,\n",
    "            test_data,\n",
    "            test_labels,\n",
    "            model_params,\n",
    "            verbose,\n",
    "            history,\n",
    "            seed=run,\n",
    "        )\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66b80f",
   "metadata": {},
   "source": [
    "# Data Corruption Experiment\n",
    "## Synthetic Dataset -- Linearly Separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087eeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower and upper bounds for data distribution PER dimension\n",
    "lows      = [-10, -10, -10, -10] + [1] # Bias added by use of [1]\n",
    "highs     = [10, 10, 10, 10] + [1]\n",
    "ins       = 5 # 4 attributes, 1 bias\n",
    "data_size = 1_200\n",
    "\n",
    "# Perceptron hyper-parameters\n",
    "model_params = {\n",
    "    'input'      : ins,\n",
    "    'eta'        : 0.5,\n",
    "    'max_iter'   : 2_000,\n",
    "    'rand_seed'  : None,\n",
    "    'ignore_flag': False,\n",
    "}\n",
    "\n",
    "'''Choose some concept to learn'''\n",
    "rng      = np.random.default_rng(42) # For reproducibility\n",
    "W        = np.concatenate([ rng.uniform(-100, 100, (ins, 1)) ])\n",
    "truth    = pn.PocketPerceptron()\n",
    "truth.pi = truth.W = W\n",
    "concept  = Concept(truth) # Concept is just a wrapper. Do truth.predict for same result\n",
    "\n",
    "'''Sample training and testing data'''\n",
    "# We sample separately the data from the uniform distribution. Then, we label according\n",
    "# to the concept (perceptron with weights W)\n",
    "data   = sample_data(lows, highs, n_samples=data_size, seed=42)\n",
    "data   = np.array(data) # Helps in keeping bucket structure\n",
    "labels = concept(data)\n",
    "\n",
    "\n",
    "history_syn_lin = perceptron_corruption_experiment(\n",
    "    X               = data,\n",
    "    y               = labels,\n",
    "    test_size       = 0.2,\n",
    "    n_buckets       = 100,\n",
    "    model_params    = model_params,\n",
    "    n_runs          = 100,\n",
    "    seed            = 42,\n",
    "    verbose         = 2\n",
    ")\n",
    "'''\n",
    "# Testing Purposes\n",
    "history_syn_lin = perceptron_corruption_experiment(\n",
    "    X               = data,\n",
    "    y               = labels,\n",
    "    test_size       = 0.2,\n",
    "    n_buckets       = 20,\n",
    "    model_params    = model_params,\n",
    "    n_runs          = 10,\n",
    "    seed            = 42,\n",
    "    verbose         = 2\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c7035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history_syn_lin).mean().plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "plt.title(\"Buckets vs Zero-One Loss; Synthetic Separable\")\n",
    "plt.xlabel(\"Number of Buckets\")\n",
    "plt.ylabel(\"Zero-One Loss\")\n",
    "plt.xticks(range(0, 51, 5))\n",
    "plt.yticks(np.linspace(0.82, 0.95, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_syn_lin).boxplot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88c17d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Synthetic Dataset -- Non-Linearly Separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd1721-ca73-4c49-aee5-a3372b5a12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower and upper bounds for data distribution PER dimension\n",
    "lows      = [-10, -10, -10, -10] + [1] # Bias added by use of [1]\n",
    "highs     = [10, 10, 10, 10] + [1]\n",
    "ins       = 5 # 4 attributes, 1 bias\n",
    "data_size = 1_200\n",
    "\n",
    "# Perceptron hyper-parameters\n",
    "model_params = {\n",
    "    'input'      : ins,\n",
    "    'eta'        : 0.5,\n",
    "    'max_iter'   : 2_000,\n",
    "    'rand_seed'  : None,\n",
    "    'ignore_flag': True,\n",
    "}\n",
    "\n",
    "'''Choose some concept to learn'''\n",
    "truth    = NPolynomial(ins, -10, 10, 42)\n",
    "concept  = Concept(truth) # Concept is just a wrapper. Do truth.predict for same result\n",
    "\n",
    "'''Sample training and testing data'''\n",
    "# We sample separately the data from the uniform distribution. Then, we label according\n",
    "# to the concept (perceptron with weights W)\n",
    "data   = sample_data(lows, highs, n_samples=data_size, seed=42)\n",
    "data   = np.array(data) # Helps in keeping bucket structure\n",
    "labels = concept(data)\n",
    "\n",
    "'''\n",
    "history_non_syn_lin = perceptron_corruption_experiment(\n",
    "    X               = data,\n",
    "    y               = labels,\n",
    "    test_size       = 0.2,\n",
    "    n_buckets       = 100,\n",
    "    model_params    = model_params,\n",
    "    n_runs          = 100,\n",
    "    seed            = 42,\n",
    "    verbose         = 2\n",
    ")\n",
    "'''\n",
    "# Testing Purposes\n",
    "history_non_syn_lin = perceptron_corruption_experiment(\n",
    "    X               = data,\n",
    "    y               = labels,\n",
    "    test_size       = 0.2,\n",
    "    n_buckets       = 20,\n",
    "    model_params    = model_params,\n",
    "    n_runs          = 10,\n",
    "    seed            = 42,\n",
    "    verbose         = 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79409676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history_non_syn_lin).mean().plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "plt.title(\"Buckets vs Zero-One Loss; synthetic Non-Separable\")\n",
    "plt.xlabel(\"Number of Buckets\")\n",
    "plt.ylabel(\"Zero-One Loss\")\n",
    "plt.xticks(range(0, 51, 5))\n",
    "plt.yticks(np.linspace(0.82, 0.95, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb423b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_non_syn_lin).boxplot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d962a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Real World Data -- Skin/No Skin Non-Separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b40b8b-2573-4fa9-b86d-5ab0510c3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and prepare data\n",
    "data = arff.loadarff('./datasets/skinNoSkin.arff')\n",
    "skin = pd.DataFrame(data[0])\n",
    "skin['bias'] = 1\n",
    "# Data cleaning\n",
    "skin.replace(b'1', -1, inplace=True)\n",
    "skin.replace(b'2', 1, inplace=True)\n",
    "data = skin.drop('Class', axis=1).assign(bias=1)\n",
    "data = data.to_numpy()\n",
    "labels = skin.Class\n",
    "\n",
    "\n",
    "\n",
    "# Perceptron hyper-parameters\n",
    "ins = data.shape[-1]\n",
    "model_params = {\n",
    "    'input'      : ins,\n",
    "    'eta'        : 0.5,\n",
    "    'max_iter'   : 2_000,\n",
    "    'rand_seed'  : None,\n",
    "    'ignore_flag': True,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "skin_history        = perceptron_corruption_experiment(\n",
    "    X               = data,\n",
    "    y               = labels,\n",
    "    test_size       = 0.2,\n",
    "    n_buckets       = 100,\n",
    "    model_params    = model_params,\n",
    "    n_runs          = 100,\n",
    "    seed            = 42,\n",
    "    verbose         = 2\n",
    ")\n",
    "'''\n",
    "# Testing Purposes\n",
    "skin_history        = perceptron_corruption_experiment(\n",
    "    X               = data[:200],\n",
    "    y               = labels[:200],\n",
    "    test_size       = 0.2,\n",
    "    n_buckets       = 20,\n",
    "    model_params    = model_params,\n",
    "    n_runs          = 10,\n",
    "    seed            = 42,\n",
    "    verbose         = 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e60003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(skin_history).mean().plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "plt.title(\"Buckets vs Zero-One Loss; Skin/No Skin\")\n",
    "plt.xlabel(\"Number of Buckets\")\n",
    "plt.ylabel(\"Zero-One Loss\")\n",
    "plt.xticks(range(0, 51, 5))\n",
    "plt.yticks(np.linspace(0.82, 0.95, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(skin_history).boxplot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83988d",
   "metadata": {},
   "source": [
    "### Save results as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skin_pickle = {\n",
    "    'history':    skin_history,\n",
    "    'n_data':     500, # We know from data-set description. \n",
    "    'test_split': 100000,\n",
    "    'n_runs':     n_runs,\n",
    "    'n_buckets':  n_buckets,\n",
    "    'max_iter':   max_iter,\n",
    "    'n_attribs':  n_attribs,\n",
    "}\n",
    "with open('skin_results.pkl', 'wb') as pkl:\n",
    "    pickle.dump(skin_pickle, pkl)\n",
    "    \n",
    "#with open('skin_pickle.pkl', 'rb') as pkl:\n",
    "#    some_dict = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f0e04",
   "metadata": {},
   "source": [
    "## Real World Data -- Iris Separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0b709-f875-4c4e-bd91-11d04e734c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "data = pd.DataFrame(iris.data)\n",
    "data['bias'] = 1\n",
    "targets = pd.DataFrame(iris.target)\n",
    "\n",
    "targets.replace(0, -1, inplace=True)\n",
    "targets.replace(1, 1, inplace=True)\n",
    "targets.replace(2, 1, inplace=True)\n",
    "\n",
    "data = data.to_numpy()\n",
    "labels = targets.to_numpy()\n",
    "\n",
    "# Perceptron hyper-parameters\n",
    "ins          = data.shape[-1]\n",
    "model_params = {\n",
    "    'input'      : ins,\n",
    "    'eta'        : 0.5,\n",
    "    'max_iter'   : 2_000,\n",
    "    'rand_seed'  : None,\n",
    "    'ignore_flag': True,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "iris_history = perceptron_corruption_experiment(\n",
    "    X               = data,\n",
    "    y               = labels,\n",
    "    test_size       = 0.2,\n",
    "    n_buckets       = 100,\n",
    "    model_params    = model_params,\n",
    "    n_runs          = 100,\n",
    "    seed            = 42,\n",
    "    verbose         = 2\n",
    ")\n",
    "'''\n",
    "# Testing Purposes\n",
    "iris_history = perceptron_corruption_experiment(\n",
    "    X               = data,\n",
    "    y               = labels,\n",
    "    test_size       = 0.2,\n",
    "    n_buckets       = 20,\n",
    "    model_params    = model_params,\n",
    "    n_runs          = 10,\n",
    "    seed            = 42,\n",
    "    verbose         = 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155ec90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(iris_history).mean().plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "plt.title(\"Buckets vs Zero-One Loss; Iris Separable\")\n",
    "plt.xlabel(\"Number of Buckets\")\n",
    "plt.ylabel(\"Zero-One Loss\")\n",
    "plt.xticks(range(0, 101, 5))\n",
    "plt.yticks(np.linspace(0.5, 1, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris_history).boxplot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc4513",
   "metadata": {},
   "source": [
    "### Save results as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "iris_pickle = {\n",
    "    'history':    iris_history,\n",
    "    'n_data':     150, # We know from data-set description. \n",
    "    'test_split': 0.2,\n",
    "    'n_runs':     n_runs,\n",
    "    'n_buckets':  n_buckets,\n",
    "    'max_iter':   max_iter,\n",
    "    'n_attribs':  4+1,\n",
    "}\n",
    "with open('iris_results.pkl', 'wb') as pkl:\n",
    "    pickle.dump(iris_pickle, pkl)\n",
    "    \n",
    "#with open('iris_results.pkl', 'rb') as pkl:\n",
    "#    some_dict = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d0e82-db08-4fbb-83a3-a9ba3dd65257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    parser = ArgumentParser(description='CoLT Experiment', formatter_class=RawTextHelpFormatter)\n",
    "    dataset_help = '''\n",
    "    Experiment to conduct. There are 4 designed and implemented.\n",
    "    \n",
    "    syn-lin: Synthetic Linearly-Separable\n",
    "    syn-non: Synthetic Non Linearly-Separable\n",
    "    iris: Iris (Linearly-Separable)\n",
    "    skin: Skin/No Skin (Non Linearly-Separable)\n",
    "    '''\n",
    "    parser.add_argument('-e', '--experiment', type=str, help=dataset_help, required=True)\n",
    "    lower_bound_help = '''\n",
    "    Lower bounds per dimension to use when using a synthetic dataset. Shall be a list of float values.\n",
    "    '''\n",
    "    parser.add_argument('-l', '--lower_bounds', nargs='+', type=float, default=[-10, -10, -10, -10], help=lower_bound_help,)\n",
    "    upper_bound_help = '''\n",
    "    Upper bounds per dimension to use when using a synthetic dataset. Shall be a list of float values.\n",
    "    '''\n",
    "    parser.add_argument('-u', '--upper_bounds', nargs='+', type=float, default=[10, 10, 10, 10], help=upper_bound_help, )\n",
    "    parser.add_argument('--bias', action='store_true', help='Flag for using bias.')\n",
    "    parser.add_argument('--dataset_size', type=int, help='Number of datapoint to sample for dataset.', required=True)\n",
    "    #\n",
    "    parser.add_argument('-t', '--test_fraction', type=float, help='Fraction of whole dataset to use as testing.', default=0.2)\n",
    "    parser.add_argument('-b', '--n_buckets', type=int, help='Number of buckets to split data into.', default=20)\n",
    "    parser.add_argument('-r', '--n_runs', type=int, help='Number of times to repeat experiment.', default=10)\n",
    "    #\n",
    "    parser.add_argument('--eta', type=float, default=1, help='Learning rate of perceptron.' )\n",
    "    parser.add_argument('--max_iter', type=int, default=1000, help='Maximum number of Perceptron iterations before convergance is assumed.')\n",
    "    parser.add_argument('--w_init', nargs='+', type=float, default=[0.5, 0.5], help='Initial weight distribution [lower, upper] bounds.')\n",
    "    # \n",
    "    parser.add_argument('-v', '--verbose',action='store_true', help='Verbosity of messages.' )\n",
    "    parser.add_argument('-i', '--index', type=int, default=0, help='Inex of experiments. Helpful when running multiple repetitions of same experiment.')\n",
    "    parser.add_argument('--result_root', type=str, default='.', help='Directory to store results.')\n",
    "    \n",
    "def obtain_data(args):\n",
    "    '''Return Data and Labels based on experiment to run'''\n",
    "    \n",
    "    experiment = args.experiment\n",
    "    # Initial distribution of perceptron weights\n",
    "    w_init_lows, w_init_highs = args.w_init\n",
    "    \n",
    "    if experiment == 'syn-lin' or experiment == 'syn-non': # Synthetic data experiment\n",
    "        '''Generate Data'''\n",
    "        # Lower and upper bounds for data distribution PER dimension\n",
    "        # Bias only added if specified!\n",
    "        lows      = args.lower_bounds + ([1] if args.bias else [])\n",
    "        highs     = args.upper_bounds + ([1] if args.bias else [])\n",
    "        assert len(lows) == len(highs), f\"upper and lower bounds do not match: {lows} vs {highs}\"\n",
    "        ins       = len(lows) # 4 attributes, 1 bias\n",
    "        data_size = args.dataset_size\n",
    "        data   = sample_data(lows, highs, n_samples=data_size, seed=42)\n",
    "        data   = np.array(data) # Helps in keeping bucket structure\n",
    "        \n",
    "        '''Select appropirate concept (linear or non-linear)'''\n",
    "        if experiment == 'syn-lin': # Use a linearly-separable concept (a perceptron)\n",
    "            rng      = np.random.default_rng(42) # For reproducibility\n",
    "            W        = np.concatenate([ rng.uniform(w_init_lows, w_init_highs, (ins, 1)) ])\n",
    "            truth    = pn.PocketPerceptron()\n",
    "            truth.pi = truth.W = W\n",
    "            # Concept is just a wrapper. Do truth.predict for same result\n",
    "            concept  = Concept(truth) \n",
    "        \n",
    "        elif experiment == 'syn-non': # Use a non-linearly-separable concept (an 'ins'-degree polynomial)\n",
    "            truth    = NPolynomial(ins, w_init_lows, w_init_highs, 42)\n",
    "            # Concept is just a wrapper. Do truth.predict for same result\n",
    "            concept  = Concept(truth) \n",
    "        else:\n",
    "            assert False, f\"Invalid experiment selected: {experiment}\"\n",
    "        \n",
    "        # Assign labels to sampled data\n",
    "        labels = concept(data)\n",
    "\n",
    "    elif experiment == 'iris':\n",
    "        # sklearn's\n",
    "        iris = datasets.load_iris()\n",
    "        data = pd.DataFrame(iris.data)\n",
    "        if args.bias:\n",
    "            data['bias'] = 1\n",
    "        targets = pd.DataFrame(iris.target)\n",
    "\n",
    "        # Separate separable and non-separable flowers\n",
    "        targets.replace(0, -1, inplace=True)\n",
    "        targets.replace(1, 1, inplace=True)\n",
    "        targets.replace(2, 1, inplace=True)\n",
    "\n",
    "        data = data.to_numpy()\n",
    "        labels = targets.to_numpy()\n",
    "    \n",
    "    elif experiment == 'skin':\n",
    "        # dataset location manually selected (change if needed)\n",
    "        data = arff.loadarff('./datasets/skinNoSkin.arff')\n",
    "        skin = pd.DataFrame(data[0])\n",
    "        if args.bias:\n",
    "            skin['bias'] = 1\n",
    "\n",
    "        # Data cleaning\n",
    "        skin.replace(b'1', -1, inplace=True)\n",
    "        skin.replace(b'2', 1, inplace=True)\n",
    "        data = skin.drop('Class', axis=1).assign(bias=1)\n",
    "        \n",
    "        data = data.to_numpy()\n",
    "        labels = skin.Class\n",
    "    else:\n",
    "        assert False, f\"Invalid experiment selected: {experiment}\"\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a0934",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # Do stuff ONLY if this is a script. Not Jupyter notebook.\n",
    "    parser = create_parser()\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Check if output file already exists  \n",
    "    fname_out = f'{args.result_root}/{args.experiment}_{args.index}_results.pkl'\n",
    "    if os.path.exists(fname_out):                                               \n",
    "            # Results file does exist: exit                                     \n",
    "            print(\"File %s already exists\"%fname_out)                           \n",
    "            return \n",
    "    \n",
    "    # Select dataset to use.\n",
    "    data, labels = obtain_data(args)\n",
    "    \n",
    "    # Perceptron learning hyper-parameters\n",
    "    model_params = {\n",
    "        'input'      : ins,\n",
    "        'eta'        : args.eta,\n",
    "        'max_iter'   : args.max_iter,\n",
    "        'rand_seed'  : None,\n",
    "        'ignore_flag': False,\n",
    "    }\n",
    "    \n",
    "    # Experiment Execution\n",
    "    history = perceptron_corruption_experiment(\n",
    "        X               = data,\n",
    "        y               = labels,\n",
    "        test_size       = args.test_fraction,\n",
    "        n_buckets       = args.n_buckets,\n",
    "        model_params    = model_params,\n",
    "        n_runs          = args.n_runs,\n",
    "        seed            = 42,\n",
    "        verbose         = args.verbose\n",
    "    )\n",
    "    \n",
    "    # Save experiment\n",
    "    pickle = {\n",
    "        'history':    history,\n",
    "        'n_data':     data.shape[0], # We know from data-set description. \n",
    "        'test_split': args.test_fraction,\n",
    "        'n_runs':     args.n_runs,\n",
    "        'n_buckets':  args.n_buckets,\n",
    "        'max_iter':   args.max_iter,\n",
    "        'n_attribs':  data.shape[1],\n",
    "    }\n",
    "    with open(fname_out, 'wb') as pkl:\n",
    "        pickle.dump(pickle, pkl)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
